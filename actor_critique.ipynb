{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30627bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f857fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.linear3 = nn.Linear(128, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.bn1(self.linear1(state)))\n",
    "        output = self.linear3(output)\n",
    "        distribution = F.softmax(output, dim=-1)\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.linear3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.bn1(self.linear1(state)))\n",
    "        # output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value\n",
    "\n",
    "def select_action(network, state):\n",
    "    ''' Selects an action given current state\n",
    "    Args:\n",
    "    - network (Torch NN): network to process state\n",
    "    - state (Array): Array of action space in an environment\n",
    "\n",
    "    Return:\n",
    "    - (int): action that is selected\n",
    "    - (float): log probability of selecting that action given state and network\n",
    "    '''\n",
    "\n",
    "    #convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "    # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "    #use network to predict action probabilities\n",
    "    print(network(state))\n",
    "    action_probs = network(state)\n",
    "    state = state.detach()\n",
    "\n",
    "    #sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "\n",
    "    #return action\n",
    "    return action, m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b692966",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1cdd15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce1e827",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(state)\n\u001b[1;32m     28\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     action, log_prob \u001b[39m=\u001b[39m select_action(network\u001b[39m=\u001b[39;49mactor, state\u001b[39m=\u001b[39;49mstate)\n\u001b[1;32m     30\u001b[0m     \u001b[39m# action = torch.argmax(actor(state))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[39m# log_prob = torch.max(actor(state))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m state_prime, reward, isTerminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[35], line 47\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(network, state)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m''' Selects an action given current state\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m- network (Torch NN): network to process state\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m- (float): log probability of selecting that action given state and network\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m#convert state to float tensor, add 1 dimension, allocate tensor on device\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# state = torch.from_numpy(state).float().unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m#use network to predict action probabilities\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39mprint\u001b[39m(network(state))\n\u001b[1;32m     48\u001b[0m action_probs \u001b[39m=\u001b[39m network(state)\n\u001b[1;32m     49\u001b[0m state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(state)))\n\u001b[1;32m     12\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear3(output)\n\u001b[1;32m     13\u001b[0m     distribution \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(output, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:301\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected 2D or 3D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim())\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "num_episodes = 1000\n",
    "num_steps = 1000\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "actor = Actor(state_size, action_size)\n",
    "critic = Critic(state_size, action_size)\n",
    "\n",
    "count_episode = range(1,num_episodes+1)\n",
    "count_actions = []\n",
    "total_count_actions = []\n",
    "total_a = 0\n",
    "for episode in range(num_episodes):\n",
    "    actor_optim = optim.Adam(actor.parameters(),lr=0.001)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=0.001)\n",
    "    state = env.reset()\n",
    "    isTerminal = False\n",
    "    score = 0\n",
    "    \n",
    "    count_a = 0\n",
    "    \n",
    "    for i in range(num_steps):       \n",
    "        count_a += 1\n",
    "        state = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob = select_action(network=actor, state=state)\n",
    "            # action = torch.argmax(actor(state))\n",
    "            # log_prob = torch.max(actor(state))\n",
    "        state_prime, reward, isTerminal, info = env.step(action.item())\n",
    "        state_prime = torch.FloatTensor(state_prime)\n",
    "        if state_prime[0] >= 0.5:\n",
    "            print(f'Num episodes {episode}, num actions {i} {isTerminal}')\n",
    "            v_next = torch.tensor([0]).float().unsqueeze(0)\n",
    "        # if isTerminal:\n",
    "        #     print(f'Num episodes {episode}, num actions {i} {isTerminal}')\n",
    "        v_curr = critic(state)\n",
    "        v_next = critic(state_prime)\n",
    "            \n",
    "\n",
    "        td_target = reward + gamma * v_next\n",
    "        td_error = reward + ((gamma*v_next)-v_curr)\n",
    "        \n",
    "        # print(v_curr)\n",
    "        # print(log_prob)\n",
    "        # Policy\n",
    "        actor_loss = (td_error)\n",
    "        actor_loss *= -log_prob\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optim.step()\n",
    "\n",
    "        # Value\n",
    "        critic_loss = F.mse_loss(td_target,v_curr)\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "        state = state_prime\n",
    "\n",
    "        # print(f'Actor loss is {actor_loss} and critic loss is {critic_loss}')\n",
    "        if state_prime[0] >= 0.5:\n",
    "            break\n",
    "        # print(state)\n",
    "    print(count_a)\n",
    "    count_actions.append(count_a)\n",
    "    total_a += count_a\n",
    "    total_count_actions.append(total_a)\n",
    "      \n",
    "torch.save(actor, 'actor.pkl')\n",
    "torch.save(critic, 'critic.pkl')\n",
    "env.close()        \n",
    "\n",
    "plt.figure()\n",
    "plt.title('Count of Episodes vs Count of Actions')\n",
    "plt.xlabel('Count of Episodes')\n",
    "plt.ylabel('Count of Actions')\n",
    "plt.plot(count_episode, count_actions)\n",
    "plt.savefig('count_actions_ac.jpg')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Total Actions vs Count of Episodes ')\n",
    "plt.ylabel('Count of Episodes')\n",
    "plt.xlabel('Total Count of Actions')\n",
    "plt.plot(total_count_actions, count_episode)\n",
    "plt.savefig('total_actions_ac.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391e8939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3171,  0.0036])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a53be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = torch.argmax(actor(state))\n",
    "    state_prime, reward, isTerminal, info = env.step(action.item())\n",
    "    state_prime = torch.FloatTensor(state_prime)\n",
    "    state = state_prime\n",
    "    # env.step(env.action_space.sample())\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc09f31dac7c0d375489b816e1f71545b84c713f14d72a59d408a359f22c77a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
